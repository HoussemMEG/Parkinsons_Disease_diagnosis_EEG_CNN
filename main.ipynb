{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTG0cPCiywDB",
        "colab_type": "code",
        "outputId": "301972d8-7c67-4547-fade-ebbc0f67c2e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Mount drive and access data\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive\n",
        "\n",
        "# Import dependencies and check TF version\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "import random\n",
        "import os\n",
        "from time import time, sleep\n",
        "import itertools\n",
        "from sklearn.utils import shuffle\n",
        "from tqdm.auto import tqdm\n",
        "# !pip install tqdm\n",
        "# check tensorflow version should be higher or equal than 2.0\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "# Check GPU (no need at every execution)\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "\n",
        "CTL = [890, 891, 892, 893, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 909, 910, 911, 912, 913, 914, 8060, 8070]\n",
        "PD = [804, 805, 806, 807, 808, 809, 810, 811, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829]\n",
        "\n",
        "# Ces données sont disponible dans les fichier REST, IMPORT_ME_REST.xlsx\n",
        "# Pour chaque MDP on a la session où il était off medication\n",
        "off_medication_session = {804: 2, 805: 2, 806: 1, 807: 1, 808: 1, 809: 2, 810: 2, 811: 2, 813: 1, 814: 2, 815: 2, 816: 1, 817: 1, 818: 2, 819: 1, \n",
        "                          820: 2, 821: 2, 822: 2, 823: 1, 824: 1, 825: 2, 826: 2, 827: 1, 828: 1, 829: 1} \n",
        "\n",
        "\n",
        "# ---------------------------------------------------------- Multiple functions ----------------------------------------------------------\n",
        "# Setting different folders for the multiple pre-processing of datasets\n",
        "def set_path(choice):\n",
        "    if choice == \"dataset_one\":\n",
        "        data_folder = \"./dataset_one/\"\n",
        "        model_folder = \"./dataset_one_models\"\n",
        "        screen_folder = \"./dataset_one_images\"\n",
        "        \n",
        "        # Time vector\n",
        "        t_min = -250 #[ms]\n",
        "        t_max = 998  #[ms]\n",
        "        nb_point = 625 \n",
        "        t = np.linspace(t_min, t_max, nb_point)\n",
        "\n",
        "    elif choice==\"dataset_two\":\n",
        "        data_folder = \"./dataset_two/\"\n",
        "        model_folder = \"./dataset_two_models\"\n",
        "        screen_folder = \"./dataset_two_images\"\n",
        "\n",
        "        # Time vector\n",
        "        t_min = -250 #[ms]\n",
        "        t_max = 998  #[ms]\n",
        "        nb_point = 625 \n",
        "        t = np.linspace(t_min, t_max, nb_point)\n",
        "\n",
        "    elif choice==\"dataset_three\":\n",
        "        data_folder = \"./dataset_three/\"\n",
        "        model_folder = \"./dataset_three_models\"\n",
        "        screen_folder = \"./dataset_three_images\"\n",
        "\n",
        "        # Time vector\n",
        "        t_min = -250 #[ms]\n",
        "        t_max = 998  #[ms]\n",
        "        nb_point = 625 \n",
        "        t = np.linspace(t_min, t_max, nb_point)\n",
        "\n",
        "\n",
        "    elif choice==\"dataset_zero\":\n",
        "        data_folder = \"./dataset_zero/\"\n",
        "        model_folder = \"./dataset_zero_models\"\n",
        "        screen_folder = \"./dataset_zero_images\"\n",
        "\n",
        "        # Time vector\n",
        "        t_min = -250 #[ms]\n",
        "        t_max = 998  #[ms]\n",
        "        nb_point = 625 \n",
        "        t = np.linspace(t_min, t_max, nb_point)\n",
        "\n",
        "    elif choice==\"dataset_five\":\n",
        "        data_folder = \"./dataset_five/\"\n",
        "        model_folder = \"./dataset_five_models\"\n",
        "        screen_folder = \"./dataset_five_images\"\n",
        "\n",
        "        # Time vector\n",
        "        t_min = -250 #[ms]\n",
        "        t_max = 998  #[ms]\n",
        "        nb_point = 625 \n",
        "        t = np.linspace(t_min, t_max, nb_point)\n",
        "\n",
        "    return data_folder, model_folder, screen_folder, nb_point\n",
        "\n",
        "    \n",
        "# Show an epoch, chan of an EEG\n",
        "def show(data, epoch, chan, save=False):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(t, data[epoch,:,chan], label='Signal EEG')\n",
        "    plt.axvline(x=0, linewidth=2, color='k')\n",
        "    plt.axhline(y=0, linewidth=0.4, color='k')\n",
        "    plt.grid(color='k', linestyle='-', linewidth=.3)\n",
        "    plt.xlabel('Temps [ms]', fontsize=12)\n",
        "    plt.ylabel('Amplitude [uV]', fontsize=12)\n",
        "    plt.gca().invert_yaxis()\n",
        "    name = 'EEG signal, chan : ' + str(chan+1) + ', epoch : ' + str(epoch+1)\n",
        "    plt.title(name, fontsize=14)\n",
        "    plt.legend(fontsize=12)\n",
        "    if save:\n",
        "        save_name = screen_folder + '/Normalized data epoch ' + str(epoch) + ' chan ' + str(chan) + '.png'\n",
        "        plt.savefig(save_name)\n",
        "        print('Images saved as :', save_name)\n",
        "\n",
        "\n",
        "# Execution time decorator\n",
        "def execution_time(function):\n",
        "    def my_function(*args, **kwargs):\n",
        "        tic = time()\n",
        "        result = function(*args, **kwargs)\n",
        "        toc = time()\n",
        "        if toc - tic > 1e-2:\n",
        "            print(\"Temps d'éxecution de\", function.__name__, \": {0:.3f} (s)\".format(toc - tic))\n",
        "        else:\n",
        "            print(\"Temps d'éxecution de\", function.__name__, \": {0:.3f} (ms)\".format(1000 * (toc - tic)))\n",
        "        return result\n",
        "    return my_function\n",
        "\n",
        "\n",
        "# Channel normalization\n",
        "def chan_normalize(data, *args):\n",
        "    # case no mean_std vector provided\n",
        "    if len(args) == 0:\n",
        "        mean_std = [] # Keep in memory for testing on new data \n",
        "        for chan in tqdm(range(data.shape[2])):\n",
        "            #mean = np.mean(data[:,:int(-t_min/2),chan])\n",
        "            mean = np.mean(data[:,:,chan])\n",
        "            data[:,:,chan] -= mean\n",
        "            std = np.std(data[:,:,chan])\n",
        "            data[:,:,chan] /= std\n",
        "\n",
        "            mean_std.append([mean, std])\n",
        "        return data, mean_std\n",
        "\n",
        "    # case where the mean_std vector is provided\n",
        "    else:\n",
        "        for chan in tqdm(range(data.shape[2])):\n",
        "            data[:,:,chan] -= args[0][chan][0]\n",
        "            data[:,:,chan] /= args[0][chan][1]\n",
        "        return data\n",
        "\n",
        "\n",
        "# Normalize data\n",
        "def normalize(data):\n",
        "    for i in range(data.shape[0]):\n",
        "        data[i,:,:] -= np.mean(data[i,:,:], axis=0)\n",
        "        data[i,:,:] /= np.std(data[i,:,:], axis=0)\n",
        "    return data\n",
        "\n",
        "\n",
        "# Display a confusion matrix\n",
        "def CM_display(confusion_matrix, normalize=True, save=False):\n",
        "    target_names = ['MP', 'CTL']\n",
        "    if len(np.shape(confusion_matrix)) == 3:\n",
        "        CM_std = np.std(confusion_matrix, axis=0)\n",
        "        confusion_matrix = np.mean(confusion_matrix, axis=0)\n",
        "        \n",
        "    accuracy = np.trace(confusion_matrix) / float(np.sum(confusion_matrix))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "\n",
        "    tick_marks = np.arange(len(target_names))\n",
        "    plt.xticks(tick_marks, target_names, rotation=45, fontsize=14)\n",
        "    plt.yticks(tick_marks, target_names, fontsize=14)\n",
        "    \n",
        "    if normalize:\n",
        "        summ = confusion_matrix.sum(axis=1)\n",
        "        confusion_matrix = 100 * confusion_matrix.astype('float') / summ[:, np.newaxis]\n",
        "        if 'CM_std' in locals():\n",
        "            CM_std = 100 * CM_std.astype('float') / summ[:, np.newaxis]\n",
        "        plt.imshow(confusion_matrix, cmap=plt.get_cmap('Blues'))\n",
        "        plt.title('Normalized confusion matrix', fontsize=16)\n",
        "        plt.clim(0, 100)\n",
        "        cbar = plt.colorbar()\n",
        "        cbar.ax.tick_params(labelsize=14)\n",
        "    else:\n",
        "        plt.imshow(confusion_matrix, cmap=plt.get_cmap('Blues'))\n",
        "        plt.title('Confusion matrix', fontsize=16)\n",
        "        cbar = plt.colorbar()\n",
        "        cbar.ax.tick_params(labelsize=14)\n",
        "\n",
        "    thresh = confusion_matrix.max()/1.5 if normalize else confusion_matrix.max()/2\n",
        "    for i, j in itertools.product(range(confusion_matrix.shape[0]), range(confusion_matrix.shape[1])):\n",
        "        if 'CM_std' in locals():\n",
        "            if normalize:\n",
        "                plt.text(j, i, \"{:.1f} % ± {:.1f} %\".format(confusion_matrix[i, j], CM_std[i, j]),\n",
        "                        horizontalalignment=\"center\",\n",
        "                        color=\"white\" if confusion_matrix[i, j] > thresh else \"black\",\n",
        "                        fontsize=14)\n",
        "            else:\n",
        "                plt.text(j, i, \"{:} ± {:}\".format(confusion_matrix[i, j], CM_std[i, j]),\n",
        "                        horizontalalignment=\"center\",\n",
        "                        color=\"white\" if confusion_matrix[i, j] > thresh else \"black\",\n",
        "                        fontsize=14)\n",
        "        else:\n",
        "            if normalize:\n",
        "                plt.text(j, i, \"{:.1f} %\".format(confusion_matrix[i, j]),\n",
        "                        horizontalalignment=\"center\",\n",
        "                        color=\"white\" if confusion_matrix[i, j] > thresh else \"black\",\n",
        "                        fontsize=14)\n",
        "            else:\n",
        "                plt.text(j, i, \"{:}\".format(confusion_matrix[i, j]),\n",
        "                        horizontalalignment=\"center\",\n",
        "                        color=\"white\" if confusion_matrix[i, j] > thresh else \"black\",\n",
        "                        fontsize=14)\n",
        "\n",
        "    plt.ylabel('True label', fontsize=16)\n",
        "    plt.xlabel('Predicted label\\naccuracy={:.1f}; misclass={:.1f}'.format(100*accuracy, 100*misclass), fontsize=16)\n",
        "\n",
        "    if save:\n",
        "        save_name = screen_folder + '/Confusion_matrix.png'\n",
        "        plt.savefig(save_name)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Calculate F1 score from a confusion matrix\n",
        "def calculate_F1_score(confusion_matrix):\n",
        "    TP = confusion_matrix[0][0] # True positive\n",
        "    TN = confusion_matrix[1][1] # True negative\n",
        "    FN = confusion_matrix[1][0] # False negative\n",
        "    FP = confusion_matrix[0][1] # False positive\n",
        "    precision = TP / float(TP + FP)\n",
        "    recall = TP / float(TP + FN)\n",
        "    F1 = 2 / (1/precision + 1/recall)\n",
        "    print(\"The F1_score is {:.1f}\\n\\n\".format(100 * F1))\n",
        "    return F1\n",
        "\n",
        "\n",
        "# Display Confidence Interval\n",
        "def plot_mean_and_CI(mean, lb, ub, legend, color_mean=None, color_shading=None):\n",
        "    # plot the shaded range of the confidence intervals\n",
        "    plt.fill_between(range(mean.shape[0]), ub, lb, color=color_shading, alpha=.3)\n",
        "                     \n",
        "    # plot the mean on top\n",
        "    plt.plot(mean, color_mean, label=legend)\n",
        "    plt.legend(fontsize=14)\n",
        "\n",
        "\n",
        "# Display accuracy with confidence interval\n",
        "def accuracy_display(history, save=False, CI_display=True):\n",
        "    train_mean = np.empty([0], dtype=float)\n",
        "    train_std = np.empty([0], dtype=float)\n",
        "    test_mean = np.empty([0], dtype=float)\n",
        "    test_std = np.empty([0], dtype=float)\n",
        "\n",
        "    for i in range(history.shape[1]):\n",
        "        train_mean = np.append(train_mean, np.expand_dims(np.mean(history[:,i,0], dtype=float), 0), axis=0)\n",
        "        test_mean = np.append(test_mean, np.expand_dims(np.mean(history[:,i,1], dtype=float), 0), axis=0)\n",
        "        train_std = np.append(train_std, np.expand_dims(np.std(history[:,i,0], dtype=float), 0), axis=0)\n",
        "        test_std = np.append(test_std, np.expand_dims(np.std(history[:,i,1], dtype=float), 0), axis=0)\n",
        "\n",
        "    train_mean *= 100\n",
        "    test_mean *= 100\n",
        "    train_std *= 100\n",
        "    test_std *= 100\n",
        "\n",
        "    plt.figure(figsize=(12,8))\n",
        "    plt.rc('xtick', labelsize=12)\n",
        "    plt.rc('ytick', labelsize=12)\n",
        "    if CI_display:\n",
        "        plot_mean_and_CI(train_mean, train_mean-train_std, train_mean+train_std, color_mean='C0', color_shading='C0', legend='Train')\n",
        "        plot_mean_and_CI(test_mean, test_mean-test_std, test_mean+test_std, color_mean='C3', color_shading='C3', legend='Test')\n",
        "    else:\n",
        "        plot_mean_and_CI(train_mean, train_mean, train_mean, color_mean='C0', color_shading='C0', legend='Train')\n",
        "        plot_mean_and_CI(test_mean, test_mean, test_mean, color_mean='C3', color_shading='C3', legend='Test')\n",
        "    plt.grid(color='k', linestyle='-', linewidth=.3)\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.ylabel('Accuracy [%]', fontsize=14)\n",
        "    plt.title('Train and test accuracy', fontsize=16)\n",
        "    plt.gca().spines['top'].set_position('zero')\n",
        "    plt.gca().spines['right'].set_position('zero')\n",
        "    plt.autoscale(enable=True, axis='x', tight=True)\n",
        "    plt.ylim(0, 100)\n",
        "    plt.yticks(np.arange(0, 110, step=10))\n",
        "    plt.xticks(np.arange(0, train_mean.shape[0], step=5))\n",
        "    if save:\n",
        "        save_name = screen_folder + '/Training_test_accuracy.png'\n",
        "        plt.savefig(save_name)\n",
        "        #save_name = screen_folder + '/Training_test_accuracy.svg'\n",
        "        #plt.savefig(save_name, format='svg', dpi=1200)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def dft(data, fe=500, lim=None, show=False, save=False):\n",
        "    N = np.shape(data)[1]\n",
        "    f = np.linspace(0, fe, N, endpoint=False)\n",
        "    f = f[0:int(N/2-1)] # unispectrale\n",
        "    fft = abs(np.fft.fft(data, axis=1))/N\n",
        "    fft[:,1:int(N/2-1),:] = 2*fft[:,1:int(N/2-1),:]\n",
        "    fft = fft[:,0:int(N/2-1),:]\n",
        "\n",
        "    if lim != None:\n",
        "        f = f[lim[0]:lim[1]]\n",
        "        fft = fft[:,lim[0]:lim[1],:]\n",
        "\n",
        "    if show:\n",
        "        # if show display a random one\n",
        "        chan = np.random.randint(61)\n",
        "        epoch = np.random.randint(data.shape[0]+1)\n",
        "        plt.figure(figsize=(10,6))\n",
        "        plt.rc('xtick', labelsize=12)\n",
        "        plt.rc('ytick', labelsize=12)\n",
        "        plt.autoscale(enable=True, axis='x', tight=True)\n",
        "        plt.plot(f, fft[epoch,:,chan])\n",
        "        #plt.xlim(0, 250)\n",
        "        plt.ylim(bottom=0)\n",
        "        plt.xlabel('Frequence [Hz]', fontsize=14)\n",
        "        plt.ylabel('Amplitude', fontsize=14)\n",
        "        title = 'Transformation de Fourier, epoch : ' + str(epoch) + ', chan : ' + str(chan)\n",
        "        plt.title(title, fontsize=16)\n",
        "        plt.grid()\n",
        "        if save:\n",
        "            save_name = screen_folder + '/' + title + '.png'\n",
        "            plt.savefig(save_name)\n",
        "            files.download(save_name)\n",
        "        plt.show()\n",
        "    return fft"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive\n",
            "2.2.0\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YViBAkDuwv6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For a faster changing in train and test set all the data are stored in a dict\n",
        "# Reading data from drive\n",
        "# At the end a dictionary is provided\n",
        "def read_data(data_folder, data_type_choice, limits, is_on_medication=False, stimuli_type=[200,201,202]):\n",
        "    data_dict = {}\n",
        "    print('Reading data')\n",
        "    # for each file in the given folder\n",
        "    for file_name in tqdm(os.listdir(data_folder)):\n",
        "        number = int(file_name.split('_')[0])\n",
        "        session = int(file_name.split('_')[1])\n",
        "    \n",
        "        if number > 850 or ((off_medication_session[number]==session) != is_on_medication): # != off medication == on medication\n",
        "            data_temp = loadmat(data_folder + '/' + file_name) \n",
        "            EEG = np.swapaxes(data_temp['EEG']['data'][0][0], 0, 2) # format [epoch][point][chan]\n",
        "            stimuli = data_temp['EEG']['epoch'][0][0][0]\n",
        "\n",
        "            # selecting stimulus type\n",
        "            indices = [index for index, element in enumerate(stimuli) if element in stimuli_type]\n",
        "            EEG = np.take(EEG, indices, axis=0)\n",
        "            target = [[1, 0]] if int(number) < 850 else [[0, 1]]  # One hot encoding\n",
        "            targets = np.tile(target, (len(indices), 1))\n",
        "        \n",
        "            if data_type_choice == 'EEG':\n",
        "                if do_normalisation: EEG = normalize(EEG) \n",
        "                data_dict[number] = {'feature' : EEG, 'target' : targets}\n",
        "\n",
        "            elif data_type_choice == 'DFT':\n",
        "                tf_data = dft(EEG)\n",
        "                if do_normalisation: tf_data = normalize(tf_data)\n",
        "                data_dict[number] = {'feature' : tf_data, 'target' : targets}\n",
        "\n",
        "            elif data_type_choice == 'DFT_lim':\n",
        "                tf_data = dft(EEG, lim=limits)\n",
        "                if do_normalisation: tf_data = normalize(tf_data)\n",
        "                data_dict[number] = {'feature' : tf_data, 'target' : targets}\n",
        "               \n",
        "            elif data_type_choice == 'DFT_mean':\n",
        "                tf_data = np.expand_dims(np.mean(dft(EEG), axis=0), axis=0)\n",
        "                if do_normalisation: tf_data = normalize(tf_data)\n",
        "                data_dict[number] = {'feature' : tf_data, 'target' : target}\n",
        "            \n",
        "            elif data_type_choice == 'DFT_lim_mean':\n",
        "                tf_data = np.expand_dims(np.mean(dft(EEG, lim=limits), axis=0), axis=0)\n",
        "                if do_normalisation: tf_data = normalize(tf_data)\n",
        "                data_dict[number] = {'feature' : tf_data, 'target' : target}\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "\n",
        "# Function that splits train and test set and corrupt data if needed\n",
        "def train_test_split(test_type, do_corruption, *args):\n",
        "    train_features = []\n",
        "    train_targets = []\n",
        "\n",
        "    # Shuffle data for a better training\n",
        "    shuffled_keys = shuffle(list(data.keys()))\n",
        "\n",
        "    # Individual train type (just 1 person in the test set)\n",
        "    if test_type == 'individual':\n",
        "        for key in shuffled_keys:\n",
        "            if key != args[0]: # args[0] = person ID\n",
        "                train_features = [*train_features, *data[key]['feature']]\n",
        "                train_targets = [*train_targets, *data[key]['target']]\n",
        "            else:\n",
        "                test_features = data[key]['feature']\n",
        "                test_targets = data[key]['target']\n",
        "\n",
        "    # Group train type (multiple persons are in test set and the others are in the train set)\n",
        "    elif test_type == 'group':\n",
        "        test_features = []\n",
        "        test_targets = []\n",
        "\n",
        "        # take random samples for test set\n",
        "        PD_test = random.sample(PD, args[0])\n",
        "        CTL_test = random.sample(CTL, args[0])\n",
        "\n",
        "        if do_corruption:\n",
        "            to_corrupt_number = int((len(data.keys())-2*args[0])*corruption_percent/100 /2) \n",
        "            PD_corrupted = random.sample([item for item in PD if item not in PD_test], to_corrupt_number)\n",
        "            CTL_corrupted = random.sample([item for item in CTL if item not in CTL_test], to_corrupt_number)\n",
        "            corrupted = [*PD_corrupted, *CTL_corrupted]\n",
        "            print('{:}% of the data are corrupted, corrupted = {:}'.format(corruption_percent, corrupted))\n",
        "\n",
        "        for key in shuffled_keys:\n",
        "            if key in [*PD_test, *CTL_test]:\n",
        "                test_features = [*test_features, *data[key]['feature']]\n",
        "                test_targets = [*test_targets, *data[key]['target']]\n",
        "            else:\n",
        "                if do_corruption:\n",
        "                    if key in corrupted:\n",
        "                        train_features = [*train_features, *data[key]['feature']]\n",
        "                        train_targets = [*train_targets, *list([1,1] - np.array(data[key]['target']))]\n",
        "                    else:\n",
        "                        train_features = [*train_features, *data[key]['feature']]\n",
        "                        train_targets = [*train_targets, *data[key]['target']]\n",
        "\n",
        "        # for item in [*PD_test, *CTL_test]:\n",
        "        #     shuffled_keys.remove(item)\n",
        "\n",
        "    train_features = np.array(train_features)\n",
        "    train_targets  = np.array(train_targets)\n",
        "    test_features = np.array(test_features)\n",
        "    test_targets = np.array(test_targets)\n",
        "    \n",
        "    # Display sets shape\n",
        "    print('\\nfeatures shape :\\n\\tTrain =', np.shape(train_features), '---- Test =', np.shape(test_features))\n",
        "    print('targets shape :\\n\\tTrain =', np.shape(train_targets), '---- Test =', np.shape(test_targets))\n",
        "    if test_type == 'group':\n",
        "        print('Selection :', PD_test, CTL_test, '\\n')\n",
        "    else:\n",
        "        print('Patient test :', [args[0]])\n",
        "\n",
        "    return train_features, train_targets, test_features, test_targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsF5idLzvnoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction model (CNN 1-D)\n",
        "class Mymodel():\n",
        "    def __init__(self, learning_rate=0.001, input_shape=[750, 60], output_shape=2, batch_size=32, show_summary=False, threshold=0.5):\n",
        "        # learning and model parameters\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "        self.loss_object = tf.keras.losses.CategoricalCrossentropy() #self.loss_object = tf.keras.losses.BinaryCrossentropy()\n",
        "        self.opt = tf.keras.optimizers.Adam(learning_rate) #self.opt = tf.keras.optimizers.SGD(self.learning_rate)\n",
        "\n",
        "        # metrics and other parameters\n",
        "        self.train_accuracy = tf.keras.metrics.BinaryAccuracy(name='Train accuracy', threshold=threshold) #self.AUC = tf.keras.metrics.AUC()\n",
        "        self.test_accuracy = tf.keras.metrics.BinaryAccuracy(name='Test accuracy', threshold=threshold)\n",
        "        self.show_summary = show_summary\n",
        "        self.history = np.asarray([[0,0]], dtype=float)\n",
        "        self.confusion_matrix = []\n",
        "        self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "        self.model = self.create_model()\n",
        "\n",
        "\n",
        "    def create_model(self):\n",
        "        # Base model\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Conv1D(filters=256, kernel_size=4, strides=1, input_shape=self.input_shape, activation='relu'))\n",
        "        model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=8, strides=2, activation='relu'))\n",
        "        model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=16, strides=4, activation='relu'))\n",
        "        model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "        model.add(tf.keras.layers.Flatten())\n",
        "        model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(self.output_shape, activation='softmax'))\n",
        "        # Other models\n",
        "        \"\"\"\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Conv1D(filters=256, kernel_size=4, strides=1, input_shape=self.input_shape, activation='relu'))\n",
        "        model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=8, strides=2, activation='relu'))\n",
        "        model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=16, strides=4, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dropout(0.5))\n",
        "        model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "        model.add(tf.keras.layers.Flatten())\n",
        "        model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dropout(0.5))\n",
        "        model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(self.output_shape, activation='softmax'))\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Conv1D(filters=256, kernel_size=4, strides=1, input_shape=self.input_shape, activation='relu'))\n",
        "        model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "        model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=8, strides=2, activation='relu'))\n",
        "        model.add(tf.keras.layers.MaxPooling1D(4))\n",
        "        #model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=16, strides=4, activation='relu'))\n",
        "        #model.add(tf.keras.layers.Dropout(0.5))\n",
        "        model.add(tf.keras.layers.Flatten())\n",
        "        model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "        #model.add(tf.keras.layers.Dropout(0.5))\n",
        "        model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(self.output_shape, activation='softmax'))\n",
        "        \"\"\"\n",
        "        model.compile(optimizer=self.opt, loss=self.loss_object, metrics=self.train_accuracy)\n",
        "        if self.show_summary:\n",
        "            model.summary()\n",
        "        return model\n",
        "\n",
        "\n",
        "    def batch_generator(self, iterable, batch_size):\n",
        "        l = len(iterable)\n",
        "        for dn in range(0, l, batch_size):\n",
        "            yield iterable[dn:min(dn + batch_size, l)]\n",
        "\n",
        "\n",
        "    def learn(self, features_train, targets_train, features_test, targets_test, batch_size, epoch, do_regulation=False):        \n",
        "        tic = time()\n",
        "        for epoch in range(epoch):\n",
        "            for batch in self.batch_generator(range(features_train.shape[0]), batch_size):\n",
        "                if do_regulation:\n",
        "                    train_prediction = self.model(features_train[batch])\n",
        "                    target_updated = controller.update_target(train_prediction, targets_train[batch], epoch)\n",
        "                    self.train(features_train[batch], target_updated)\n",
        "                    #print(\"Loss: %s\" % self.train_loss.result().numpy())\n",
        "                    controller.get_loss(self.train_loss)\n",
        "                else:\n",
        "                    self.train(features_train[batch], targets_train[batch]) \n",
        "                    #print(\"Loss: %s\" % self.train_loss.result().numpy())\n",
        "                    self.train_loss.reset_states()\n",
        "\n",
        "            test_prediction = self.test(features_test, targets_test)\n",
        "            self.update_accuracy()\n",
        "            \n",
        "            toc = time()\n",
        "            if epoch%10==9:\n",
        "                print('Epoch :{:3d}, Train accuracy : {:2.1f}%  /  Test accuracy : {:2.1f}%  /  Elapsed time : {:.1f} (s)'\n",
        "                .format(epoch+1, 100*self.history[-1,0], 100*self.history[-1,1], toc-tic))\n",
        "            tic = time()\n",
        "\n",
        "        self.calculate_confusion_matrix(test_prediction, targets_test)\n",
        "\n",
        "\n",
        "    # TF 2.X graph mode decorator\n",
        "    @tf.function\n",
        "    def train(self, features, targets):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.model(features)\n",
        "            loss = self.loss_object(targets, predictions)\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "        self.train_loss(loss)\n",
        "        self.train_accuracy.update_state(targets, predictions)\n",
        "        \n",
        "\n",
        "    # Saving accuracies in a memory and resetting them\n",
        "    def update_accuracy(self):\n",
        "        self.history = np.append(self.history, [[self.train_accuracy.result().numpy(), self.test_accuracy.result().numpy()]], axis=0)\n",
        "        self.train_accuracy.reset_states()\n",
        "        self.test_accuracy.reset_states()\n",
        "\n",
        "\n",
        "    def test(self, features_test, targets_test):\n",
        "        batch_size = min(300, features_test.shape[0])\n",
        "        predictions = np.empty([0, 2], dtype=np.uint8)\n",
        "\n",
        "        for batch in self.batch_generator(range(features_test.shape[0]), batch_size):\n",
        "            prediction = self.model(features_test[batch])\n",
        "            predictions = np.append(predictions, prediction, axis=0)\n",
        "\n",
        "        self.test_accuracy.update_state(targets_test, predictions)\n",
        "        return predictions\n",
        "\n",
        "\n",
        "    def calculate_confusion_matrix(self, predictions, targets_test):\n",
        "        # 0 for PD, 1 for CTL  \n",
        "        new_target = [0 if tuple(target)==(1,0) else 1 for target in targets_test]\n",
        "        new_prediction = [0 if prediction[0]>0.5 else 1 for prediction in predictions]\n",
        "        self.confusion_matrix = tf.math.confusion_matrix(new_target, new_prediction)\n",
        "\n",
        "    \n",
        "    def load_model(self, load_name='model'):\n",
        "        try:\n",
        "            self.model.load_weights(model_folder + '/' + load_name + '_weights.h5')\n",
        "            print('...Model loaded...')\n",
        "        except:\n",
        "            print('...No model to load...')\n",
        "\n",
        "\n",
        "    def save_model(self, save_name='model'):\n",
        "        self.model.save(model_folder + '/my_model.h5')\n",
        "        self.model.save_weights(model_folder + '/' + save_name + '_weights.h5')\n",
        "        print('...Model saved...')\n",
        "\n",
        "\n",
        "class control():\n",
        "    def __init__(self, loss_memory_size=10):\n",
        "        self.loss_memory_size = loss_memory_size\n",
        "        self.loss_memory = [1.]\n",
        "        self.betta_max = 0.7\n",
        "        self.alpha = 50 \n",
        "        self.betta = 0  #[*np.linspace(0,1, 15), *85*[1]]\n",
        "\n",
        "    def update_target(self, prediction, target, epoch):\n",
        "        count = 0\n",
        "        self.update_betta()\n",
        "        # for i in range(prediction.shape[0]):\n",
        "        #     print('loss = {:.4}, betta = {:.2} prediction = {:}, target = {:}, is_equal = {:}'\n",
        "        #     .format(float(self.loss_memory[-1]), float(self.betta), np.round(prediction[i],1), target[i], list(target[i])==list(np.round(prediction[i],0))))\n",
        "        #     if list(target[i])==list(np.round(prediction[i],0)):\n",
        "        #         count += 1\n",
        "        # print('COUNT : ', count)\n",
        "        target_updated = np.array(np.round(self.betta*prediction + (1-self.betta)*target, 2), dtype=int)\n",
        "        return target_updated\n",
        "    \n",
        "    def get_loss(self, loss):\n",
        "        self.loss_memory.append(loss.result().numpy())\n",
        "\n",
        "    def update_betta(self):\n",
        "        self.betta = min(1*np.exp(-self.alpha * self.loss_memory[-1]), self.betta_max) #saturer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmsnkoRoVQbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data reading\n",
        "dataset_choice = 'dataset_three'        # dataset name\n",
        "testing_case = 'group'                  # 'group' / 'individual'\n",
        "data_type_choice = 'DFT_mean'           # choose : 'EEG' 'DFT' 'DFT_lim' 'DFT_mean' 'DFT_lim_mean'\n",
        "stimuli_type = [202]                    # 201 : Standard / 200 : Target / 202 : Novel (possibility to choose a combination of the three)\n",
        "corruption_percent = 30                 # Percentage of corrupted data in train set : 5 10 20 30 40 50 70 90 100\n",
        "limits = [0, 80]                        # In dft case take only few features (not all dft weights) [0, 80]\n",
        "fs = 500                                # Sampling frequency\n",
        "do_normalisation = True                 # Normalize data bool\n",
        "do_corruption = True                    # Corrupt data bool (swap label)\n",
        "do_regulation = False                   # Let the controller do regulation (testing it right now)\n",
        "\n",
        "# Execution parameters\n",
        "repeat_training = 30                    # Repeat multiple times the training and test (we only have 50 patients)\n",
        "epoch = 50                             \n",
        "batch_size = 4                         \n",
        "test_size = 5                           # Number of persons from PD and CTL to take in the train set (5 means a test set of 10)\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "# Set paths and read data\n",
        "data_folder, model_folder, screen_folder, nb_point = set_path(dataset_choice)\n",
        "data = read_data(data_folder, data_type_choice, stimuli_type=stimuli_type, limits=limits)\n",
        "if data_type_choice.split('_')[0]=='DFT':\n",
        "    print('Index limits = {:} --> freq = ({:.1f}, {:.1f})'.format(limits, limits[0]*fs/nb_point, limits[1]*fs/nb_point))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uepTIWuKORzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Metrics containers\n",
        "accuracy = []\n",
        "F1 = []\n",
        "confusion_matrix = []\n",
        "\n",
        "if testing_case == 'group':\n",
        "    for run in range(repeat_training):\n",
        "        print('Run number :', run+1)\n",
        "        # Training\n",
        "        features_train, targets_train, features_test, targets_test = train_test_split(testing_case, do_corruption, test_size)\n",
        "        controller = control() \n",
        "        model = Mymodel(learning_rate, input_shape=features_train.shape[1:])\n",
        "        model.learn(features_train, targets_train, features_test, targets_test, batch_size, epoch, do_regulation)\n",
        "        # Display result \n",
        "        F1.append(calculate_F1_score(np.array(model.confusion_matrix)))\n",
        "        confusion_matrix = [*confusion_matrix, *[model.confusion_matrix]]\n",
        "        accuracy = [*accuracy, *[model.history]]\n",
        "\n",
        "    print('\\n\\n', 50*'-' , 'RESULTS', 50*'-')\n",
        "    print('The F1_score is : {:.1f}% ± {:.1f}%'.format(100*np.mean(F1), 100*np.std(F1)))\n",
        "    CM_display(np.array(confusion_matrix), save=True, normalize=True)\n",
        "    accuracy_display(np.array(accuracy), save=True)\n",
        "\n",
        "if testing_case == 'individual':\n",
        "    for patient in [*PD, *CTL]:\n",
        "        patient_accuracy = []\n",
        "        features_train, targets_train, features_test, targets_test = train_test_split(testing_case, do_corruption, patient)\n",
        "\n",
        "        for run in range(repeat_training):\n",
        "            print('Run number :', run+1)\n",
        "            # Training \n",
        "            model = Mymodel(learning_rate, threshold=threshold, input_shape=features_train.shape[1:])\n",
        "            model.learn(features_train, targets_train, features_test, targets_test, batch_size, epoch, do_regulation)\n",
        "            accuracy = [*accuracy, *[model.history]]\n",
        "\n",
        "    accuracy_display(np.array(accuracy), save=True, CI_display=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wegtvg8kpqaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the saved images\n",
        "files.download(screen_folder + '/Confusion_matrix.png')\n",
        "files.download(screen_folder + '/Training_test_accuracy.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBLAXv54D31R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
